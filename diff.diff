--- ./tcp_bbr.c	2021-08-12 16:15:06.530999944 +0800
+++ ./tcp_bbr.c.new	2021-08-17 11:19:58.623841452 +0800
@@ -86,8 +86,12 @@
 /* BBR congestion control block */
 struct bbr {
 	u32	min_rtt_us;	        /* min RTT in min_rtt_win_sec window */
+	u32	prev_min_rtt_us;
+	u32	rtt_us;	        /* min RTT in min_rtt_win_sec window */
 	u32	min_rtt_stamp;	        /* timestamp of min_rtt_us */
 	u32	probe_rtt_done_stamp;   /* end time for BBR_PROBE_RTT mode */
+	u32     probe_bw_steady_ms;
+	u32     probe_bw_steady_stamp;
 	struct minmax bw;	/* Max recent delivery rate in pkts/uS << 24 */
 	u32	rtt_cnt;	    /* count of packet-timed rounds elapsed */
 	u32     next_rtt_delivered; /* scb->tx.delivered at end of round */
@@ -112,7 +116,9 @@
 		full_bw_cnt:2,	/* number of rounds without large bw gains */
 		cycle_idx:3,	/* current index in pacing_gain cycle array */
 		has_seen_rtt:1, /* have we seen an RTT sample yet? */
-		unused_b:5;
+		quick_probe:1,
+		probe_rtt_cnt:2,
+		unused_b:2;
 	u32	prior_cwnd;	/* prior cwnd upon entering loss recovery */
 	u32	full_bw;	/* recent bw, to estimate if pipe is full */

@@ -125,17 +131,21 @@
 		unused_c:6;
 };

-#define CYCLE_LEN	8	/* number of phases in a pacing gain cycle */
+#define CYCLE_LEN	3	/* number of phases in a pacing gain cycle */

 /* Window length of bw filter (in rounds): */
 static const int bbr_bw_rtts = CYCLE_LEN + 2;
 /* Window length of min_rtt filter (in sec): */
 static const u32 bbr_min_rtt_win_sec = 10;
 /* Minimum time (in ms) spent at bbr_cwnd_min_target in BBR_PROBE_RTT mode: */
-static const u32 bbr_probe_rtt_mode_ms = 200;
+static const u32 bbr_probe_rtt_mode_ms = 20;
+static const u32 bbr_probe_rtt_cnt = 3;
 /* Skip TSO below the following bandwidth (bits/sec): */
 static const int bbr_min_tso_rate = 1200000;

+static const u32 bbr_probe_bw_steady_ms = 120;
+static const u32 bbr_probe_bw_steady_long_ms = 600;
+
 /* Pace at ~1% below estimated bw, on average, to reduce queue at bottleneck.
  * In order to help drive the network toward lower queues and low latency while
  * maintaining high utilization, the average pacing rate aims to be slightly
@@ -158,13 +168,12 @@
 static const int bbr_cwnd_gain  = BBR_UNIT * 2;
 /* The pacing_gain values for the PROBE_BW gain cycle, to discover/share bw: */
 static const int bbr_pacing_gain[] = {
-	BBR_UNIT * 5 / 4,	/* probe for more available bw */
+	BBR_UNIT * 9 / 8,	/* probe for more available bw */
 	BBR_UNIT * 3 / 4,	/* drain queue and/or yield bw to other flows */
-	BBR_UNIT, BBR_UNIT, BBR_UNIT,	/* cruise at 1.0*bw to utilize pipe, */
-	BBR_UNIT, BBR_UNIT, BBR_UNIT	/* without creating excess queue... */
-};
+	BBR_UNIT,		/* cruise at 1.0*bw to utilize pipe, without */
+};				/* creating excess queue... */
 /* Randomize the starting gain cycling phase over N phases: */
-static const u32 bbr_cycle_rand = 7;
+static const u32 bbr_cycle_rand = CYCLE_LEN - 1;

 /* Try to keep at least this many packets in flight, if things go smoothly. For
  * smooth functioning, a sliding window protocol ACKing every other packet
@@ -354,9 +363,8 @@
  * measurements (e.g., delayed ACKs or other ACK compression effects). This
  * noise may cause BBR to under-estimate the rate.
  */
-static u32 bbr_bdp(struct sock *sk, u32 bw, int gain)
+static u32 bbr_bdp(struct sock *sk, u32 bw, u32 rtt_us, int gain)
 {
-	struct bbr *bbr = inet_csk_ca(sk);
 	u32 bdp;
 	u64 w;

@@ -366,10 +374,10 @@
 	 * ACKed so far. In this case, an RTO can cut cwnd to 1, in which
 	 * case we need to slow-start up toward something safe: TCP_INIT_CWND.
 	 */
-	if (unlikely(bbr->min_rtt_us == ~0U))	 /* no valid RTT samples yet? */
+	if (unlikely(rtt_us == ~0U))	 /* no valid RTT samples yet? */
 		return TCP_INIT_CWND;  /* be safe: cap at default initial cwnd*/

-	w = (u64)bw * bbr->min_rtt_us;
+	w = (u64)bw * rtt_us;

 	/* Apply a gain to the given value, remove the BW_SCALE shift, and
 	 * round the value up to avoid a negative feedback loop.
@@ -409,9 +417,10 @@
 /* Find inflight based on min RTT and the estimated bottleneck bandwidth. */
 static u32 bbr_inflight(struct sock *sk, u32 bw, int gain)
 {
+	struct bbr *bbr = inet_csk_ca(sk);
 	u32 inflight;

-	inflight = bbr_bdp(sk, bw, gain);
+	inflight = bbr_bdp(sk, bw, bbr->min_rtt_us, gain);
 	inflight = bbr_quantization_budget(sk, inflight);

 	return inflight;
@@ -526,7 +535,13 @@
 	if (bbr_set_cwnd_to_recover_or_restore(sk, rs, acked, &cwnd))
 		goto done;

-	target_cwnd = bbr_bdp(sk, bw, gain);
+	if (!bbr->quick_probe) {
+		target_cwnd = bbr_bdp(sk, bw, bbr->min_rtt_us, gain);
+	} else {
+		target_cwnd = bbr_bdp(sk, bw, bbr->prev_min_rtt_us, gain);
+		bbr->quick_probe = 0;
+		bbr->prev_min_rtt_us = 0;
+	}

 	/* Increment the cwnd to account for excess ACKed data that seems
 	 * due to aggregation (of data and/or ACKs) visible in the ACK stream.
@@ -556,13 +571,27 @@
 	bool is_full_length =
 		tcp_stamp_us_delta(tp->delivered_mstamp, bbr->cycle_mstamp) >
 		bbr->min_rtt_us;
+	bool filter_expired;
 	u32 inflight, bw;

 	/* The pacing_gain of 1.0 paces at the estimated bw to try to fully
 	 * use the pipe without increasing the queue.
 	 */
-	if (bbr->pacing_gain == BBR_UNIT)
-		return is_full_length;		/* just use wall clock time */
+	if (bbr->pacing_gain == BBR_UNIT) {
+		filter_expired = after(tcp_jiffies32,
+			       bbr->probe_bw_steady_stamp +
+			       msecs_to_jiffies(bbr->probe_bw_steady_ms));
+		if (bbr->min_rtt_us >= bbr->probe_bw_steady_ms*500)
+			filter_expired = after(tcp_jiffies32,
+				       bbr->probe_bw_steady_stamp +
+				       bbr_probe_bw_steady_long_ms);
+		if (rs->rtt_us >= 0 && rs->rtt_us <= bbr->min_rtt_us) {
+			bbr->quick_probe = 1;
+			bbr->prev_min_rtt_us = bbr->min_rtt_us;
+		}
+
+		return filter_expired || bbr->quick_probe;
+	}

 	inflight = bbr_packets_in_net_at_edt(sk, rs->prior_in_flight);
 	bw = bbr_max_bw(sk);
@@ -581,8 +610,8 @@
 	 * probing didn't find more bw. If inflight falls to match BDP then we
 	 * estimate queue is drained; persisting would underutilize the pipe.
 	 */
-	return is_full_length ||
-		inflight <= bbr_inflight(sk, bw, BBR_UNIT);
+	// drain to target
+	return inflight <= bbr_inflight(sk, bw, BBR_UNIT);
 }

 static void bbr_advance_cycle_phase(struct sock *sk)
@@ -590,8 +619,11 @@
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct bbr *bbr = inet_csk_ca(sk);

-	bbr->cycle_idx = (bbr->cycle_idx + 1) & (CYCLE_LEN - 1);
+	bbr->cycle_idx = bbr->quick_probe ? 0 :
+			 (bbr->cycle_idx + 1) & (CYCLE_LEN - 1);
 	bbr->cycle_mstamp = tp->delivered_mstamp;
+	if (bbr->cycle_idx == CYCLE_LEN - 1)
+		bbr->probe_bw_steady_stamp = tcp_jiffies32;
 }

 /* Gain cycling: cycle pacing gain to converge to fair share of available bw. */
@@ -617,6 +649,8 @@

 	bbr->mode = BBR_PROBE_BW;
 	bbr->cycle_idx = CYCLE_LEN - 1 - prandom_u32_max(bbr_cycle_rand);
+	if (bbr->cycle_idx == CYCLE_LEN - 1)
+		bbr->probe_bw_steady_stamp = tcp_jiffies32;
 	bbr_advance_cycle_phase(sk);	/* flip to next phase of gain cycle */
 }

@@ -771,6 +805,8 @@
 		bbr->rtt_cnt++;
 		bbr->round_start = 1;
 		bbr->packet_conservation = 0;
+		if (bbr->mode == BBR_PROBE_RTT)
+			bbr->probe_rtt_cnt++;
 	}

 	bbr_lt_bw_sampling(sk, rs);
@@ -793,8 +829,10 @@
 	 * phase when app writes faster than the network can deliver :)
 	 */
 	if (!rs->is_app_limited || bw >= bbr_max_bw(sk)) {
+		u32 win = 4 + div64_long(bbr_probe_bw_steady_ms*1000,
+					     bbr->min_rtt_us);
 		/* Incorporate new sample into our max bw filter. */
-		minmax_running_max(&bbr->bw, bbr_bw_rtts, bbr->rtt_cnt, bw);
+		minmax_running_max(&bbr->bw, win, bbr->rtt_cnt, bw);
 	}
 }

@@ -911,6 +949,10 @@
 	      after(tcp_jiffies32, bbr->probe_rtt_done_stamp)))
 		return;

+	if (bbr->probe_rtt_cnt < bbr_probe_rtt_cnt)
+		return;
+
+	bbr->probe_rtt_cnt = 0;
 	bbr->min_rtt_stamp = tcp_jiffies32;  /* wait a while until PROBE_RTT */
 	tp->snd_cwnd = max(tp->snd_cwnd, bbr->prior_cwnd);
 	bbr_reset_mode(sk);
@@ -944,6 +986,7 @@
 	/* Track min RTT seen in the min_rtt_win_sec filter window: */
 	filter_expired = after(tcp_jiffies32,
 			       bbr->min_rtt_stamp + bbr_min_rtt_win_sec * HZ);
+	bbr->rtt_us = rs->rtt_us;
 	if (rs->rtt_us >= 0 &&
 	    (rs->rtt_us < bbr->min_rtt_us ||
 	     (filter_expired && !rs->is_ack_delayed))) {
@@ -954,6 +997,7 @@
 	if (bbr_probe_rtt_mode_ms > 0 && filter_expired &&
 	    !bbr->idle_restart && bbr->mode != BBR_PROBE_RTT) {
 		bbr->mode = BBR_PROBE_RTT;  /* dip, drain queue */
+		bbr->probe_rtt_cnt = 0;
 		bbr_save_cwnd(sk);  /* note cwnd so we can restore it */
 		bbr->probe_rtt_done_stamp = 0;
 	}
@@ -981,6 +1025,20 @@
 		bbr->idle_restart = 0;
 }

+static u32 get_pacing_gain(struct sock *sk)
+{
+	struct bbr *bbr = inet_csk_ca(sk);
+	u32 gain = bbr_pacing_gain[bbr->cycle_idx];
+
+	if (bbr->cycle_idx == 0) {
+		gain *= bbr->rtt_us;
+		gain = (u32)div64_long(gain, bbr->min_rtt_us);
+	}
+	// because of drain-to-target, there is no need to drain symmetrically.
+
+	return gain;
+}
+
 static void bbr_update_gains(struct sock *sk)
 {
 	struct bbr *bbr = inet_csk_ca(sk);
@@ -997,7 +1055,7 @@
 	case BBR_PROBE_BW:
 		bbr->pacing_gain = (bbr->lt_use_bw ?
 				    BBR_UNIT :
-				    bbr_pacing_gain[bbr->cycle_idx]);
+				    get_pacing_gain(sk));
 		bbr->cwnd_gain	 = bbr_cwnd_gain;
 		break;
 	case BBR_PROBE_RTT:
@@ -1062,6 +1120,7 @@
 	bbr->full_bw_cnt = 0;
 	bbr->cycle_mstamp = 0;
 	bbr->cycle_idx = 0;
+	bbr->probe_rtt_cnt = 0;
 	bbr_reset_lt_bw_sampling(sk);
 	bbr_reset_startup_mode(sk);
